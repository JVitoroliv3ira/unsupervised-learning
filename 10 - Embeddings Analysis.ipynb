{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dcc22e8b",
      "metadata": {
        "id": "dcc22e8b"
      },
      "source": [
        "# Análise de Embeddings e Redução da Dimensionalidade\n",
        "\n",
        "**Objetivo.** Dado um conjunto de textos, gerar embeddings com BERT e investigar a estrutura dos dados via PCA, t-SNE e UMAP. Em seguida, identificar clusters e relacioná-los a categorias semânticas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3272b0d4",
      "metadata": {
        "id": "3272b0d4"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    'I swap butter for olive oil in many recipes.',\n",
        "    'Canberra is the capital of Australia.',\n",
        "    'Ottawa is the capital city of Canada.',\n",
        "    'Paris is the most populated city in France.',\n",
        "    'Tokyo is among the most populous metropolitan areas worldwide.',\n",
        "    'I prefer my coffee with no sugar and a splash of milk.',\n",
        "    'The recipe for pasta carbonara is simple.',\n",
        "    'A pinch of salt enhances sweetness in desserts.',\n",
        "    'Alignment techniques reduce harmful outputs.',\n",
        "    'Explainable AI highlights salient features for decisions.',\n",
        "    'Transformer models enable long-range language dependencies.',\n",
        "    'Black swan events stress-test portfolio resilience.',\n",
        "    'The Sahara Desert spans much of North Africa.',\n",
        "    'Inflation erodes real purchasing power of cash.',\n",
        "    'Aromatics like garlic and onion build flavor early.',\n",
        "    'Value stocks trade at lower multiples relative to fundamentals.',\n",
        "    'Quantization reduces memory with minimal accuracy loss.',\n",
        "    'Tax-loss harvesting offsets capital gains.',\n",
        "    'Investing in technology can be risky.',\n",
        "    'Fermented foods add acidity and complexity.',\n",
        "    'Marinating tofu improves texture and taste.',\n",
        "    'Vector databases power semantic search at scale.',\n",
        "    'Distillation transfers knowledge from large to small models.',\n",
        "    'The Great Barrier Reef lies off Australia’s northeast coast.',\n",
        "    'Retrieval-augmented generation grounds answers in sources.',\n",
        "    'Iceland lies on the Mid-Atlantic Ridge.',\n",
        "    'The Baltic states border the eastern Baltic Sea.',\n",
        "    'Multimodal learning aligns text with images and audio.',\n",
        "    'Risk tolerance should guide position sizing.',\n",
        "    'Time in the market beats timing the market.',\n",
        "    'Behavioral biases can derail investment plans.',\n",
        "    'Reinforcement learning fine-tunes policies from human feedback.',\n",
        "    'Edge AI runs models under strict latency constraints.',\n",
        "    'Deglazing lifts browned bits to make pan sauces.',\n",
        "    'Tempering chocolate stabilizes cocoa butter crystals.',\n",
        "    'What is the capital of France?',\n",
        "    'Johannesburg is a major city but not South Africa’s capital.',\n",
        "    'The Danube passes through multiple European capitals.',\n",
        "    'The Amazon River carries one of the largest water volumes on Earth.',\n",
        "    'A healthy emergency fund reduces forced selling.',\n",
        "    'I batch-cook grains for quick lunches.',\n",
        "    'Resting steak helps redistribute the juices.',\n",
        "    'The Atacama is one of the driest deserts on the planet.',\n",
        "    'Liquidity risk rises when trading volumes are thin.',\n",
        "    'Mount Everest is the highest peak above sea level.',\n",
        "    'Graph neural networks capture relational structure.',\n",
        "    'Sourdough starter needs regular feedings to stay active.',\n",
        "    'The stock market experienced a drop today.',\n",
        "    'Umami-rich ingredients deepen savory dishes.',\n",
        "    'Al dente pasta retains a slight bite after cooking.',\n",
        "    'Rebalancing restores target asset allocation.',\n",
        "    'Continual learning mitigates catastrophic forgetting.',\n",
        "    'Bond duration measures sensitivity to interest-rate changes.',\n",
        "    'Diffusion models synthesize high-fidelity images.',\n",
        "    'Expense ratios compound against long-term returns.',\n",
        "    'Self-supervised pretraining reduces labeled data needs.',\n",
        "    'What country contains the city of Kyoto?',\n",
        "    'Stir-frying requires high heat and constant movement.',\n",
        "    'Covered calls generate income with capped upside.',\n",
        "    'The Nile flows northward into the Mediterranean Sea.',\n",
        "    'Causal inference distinguishes correlation from effect.',\n",
        "    'Prompt engineering steers generative behavior reliably.',\n",
        "    'Few-shot prompting improves generalization on new tasks.',\n",
        "    'Growth investing prioritizes earnings expansion.',\n",
        "    'The Alps stretch across several central European countries.',\n",
        "    'The Andes form a continuous mountain range along South America.',\n",
        "    'I cook vegetarian meals on weekdays to simplify planning.',\n",
        "    'Natural language processing has advanced greatly.',\n",
        "    'Sous-vide delivers precise temperature control.',\n",
        "    'Diversification reduces idiosyncratic risk across holdings.',\n",
        "    'Sharpe ratio evaluates risk-adjusted performance.',\n",
        "    'Artificial intelligence is transforming the world.',\n",
        "    'Credit spreads widen during economic uncertainty.',\n",
        "    'Emerging markets add diversification but higher volatility.',\n",
        "    'Mise en place speeds up weeknight cooking.',\n",
        "    'The Caspian Sea is a landlocked body of water.',\n",
        "    'Evaluation with benchmarks must avoid data leakage.',\n",
        "    'Cairo sits along the Nile River delta.',\n",
        "    'Federated learning trains models without centralizing data.',\n",
        "    'Lagos is Nigeria’s largest city by population.',\n",
        "    'Dollar-cost averaging smooths entry price over time.',\n",
        "    'LoRA adapters enable efficient fine-tuning.',\n",
        "    'I keep a jar of homemade pesto for pasta.',\n",
        "    'New Delhi serves as the seat of India’s government.',\n",
        "    'I like to cook Italian dishes on Sundays.',\n",
        "    'Roasting vegetables caramelizes natural sugars.',\n",
        "    'ETFs provide broad market exposure with intraday liquidity.',\n",
        "    'Proofing time affects a bread’s crumb structure.'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2de82112",
      "metadata": {
        "id": "2de82112"
      },
      "source": [
        "## Predição dos Embeddings\n",
        "\n",
        "Utilize o modelo BERT pré-treinado para gerar embeddings de todos os textos fornecidos.  \n",
        "O objetivo é obter uma matriz `X` com formato **(N, dim)**, onde **N** é o número de textos e **dim** é a dimensionalidade dos vetores de embedding."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install sentence-transformers torch scikit-learn matplotlib seaborn pandas umap-learn -q"
      ],
      "metadata": {
        "id": "F3O94a4vP07c"
      },
      "id": "F3O94a4vP07c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6767a199",
      "metadata": {
        "id": "6767a199"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "\n",
        "nome_do_modelo = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "modelo = SentenceTransformer(nome_do_modelo, device=device)\n",
        "\n",
        "X = modelo.encode(sentences, convert_to_numpy=True)\n",
        "\n",
        "print(f'Quantidade de frases: {len(sentences)}')\n",
        "print(f'Shape da matriz: {X.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f497cb",
      "metadata": {
        "id": "f6f497cb"
      },
      "source": [
        "## PCA\n",
        "\n",
        "Aplique **PCA (Principal Component Analysis)** para projetar os embeddings em duas dimensões e visualizar a estrutura global dos dados.  \n",
        "O PCA ajuda a capturar as direções de maior variância e pode indicar agrupamentos lineares.\n",
        "\n",
        "**Tarefas:**\n",
        "- Reduza a dimensionalidade dos embeddings para 2 componentes principais.  \n",
        "- Plote os pontos resultantes com `matplotlib`, identificando possíveis agrupamentos.  \n",
        "- Analise qualitativamente se há separação entre textos de temas distintos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a63c2fa",
      "metadata": {
        "id": "9a63c2fa"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA(n_components=2, random_state=314)\n",
        "\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], s=30, alpha=0.8)\n",
        "\n",
        "plt.title(\"PCA dos embeddings (2D)\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25bebfa9",
      "metadata": {
        "id": "25bebfa9"
      },
      "source": [
        "## t-SNE\n",
        "\n",
        "Use **t-SNE (t-distributed Stochastic Neighbor Embedding)** para investigar a estrutura local dos dados.  \n",
        "Diferente do PCA, o t-SNE tenta preservar vizinhanças locais e pode revelar grupos mais sutis.\n",
        "\n",
        "**Tarefas:**\n",
        "- Reduza os embeddings para 2D usando `TSNE` do `scikit-learn`.  \n",
        "- Ajuste parâmetros como `perplexity` e `learning_rate` para comparar resultados.  \n",
        "- Visualize o mapa e observe se os textos semelhantes ficam próximos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f19eeb6b",
      "metadata": {
        "id": "f19eeb6b"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "configuracoes = {\n",
        "  \"per_5_lr_50\": { \"perplexity\": 5, \"learning_rate\": 50 },\n",
        "  \"per_10_lr_70\": {\"perplexity\": 10, \"learning_rate\": 70},\n",
        "  \"per_15_lr_90\": {\"perplexity\": 15, \"learning_rate\": 90},\n",
        "}\n",
        "\n",
        "n_plots = len(configuracoes)\n",
        "fig, axes = plt.subplots(1, n_plots, figsize=(6 * n_plots, 5), squeeze=False)\n",
        "\n",
        "for (name, params), ax in zip(configuracoes.items(), axes[0]):\n",
        "    tsne = TSNE(\n",
        "        n_components=2,\n",
        "        init=\"random\",\n",
        "        random_state=42,\n",
        "        **params,\n",
        "    )\n",
        "\n",
        "    X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "    ax.scatter(\n",
        "        X_tsne[:, 0],\n",
        "        X_tsne[:, 1],\n",
        "        s=40,\n",
        "        alpha=0.8,\n",
        "    )\n",
        "\n",
        "    ax.set_title(\n",
        "        f\"{name}\\nperp={params['perplexity']}, lr={params['learning_rate']}\"\n",
        "    )\n",
        "    ax.set_xlabel(\"t-SNE 1\")\n",
        "    ax.set_ylabel(\"t-SNE 2\")\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f7829b7",
      "metadata": {
        "id": "4f7829b7"
      },
      "source": [
        "## UMAP\n",
        "\n",
        "Aplique **UMAP (Uniform Manifold Approximation and Projection)** como alternativa ao t-SNE.  \n",
        "O UMAP é mais eficiente, preserva parte da estrutura global e é útil para visualização e pré-processamento.\n",
        "\n",
        "**Tarefas:**\n",
        "- Gere uma projeção 2D dos embeddings com `umap.UMAP`.  \n",
        "- Experimente variar `n_neighbors` e `min_dist` para observar mudanças na distribuição dos clusters.  \n",
        "- Compare visualmente com os resultados do PCA e t-SNE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ad9569",
      "metadata": {
        "id": "17ad9569"
      },
      "outputs": [],
      "source": [
        "from umap import UMAP\n",
        "\n",
        "configuracoes = {\n",
        "    \"nn_5_md_0.5\":  {\"n_neighbors\": 5,  \"min_dist\": 0.5},\n",
        "    \"nn_15_md_0.10\": {\"n_neighbors\": 15, \"min_dist\": 0.10},\n",
        "    \"nn_30_md_0.1\": {\"n_neighbors\": 30, \"min_dist\": 0.1},\n",
        "}\n",
        "\n",
        "n_plots = len(configuracoes)\n",
        "fig, axes = plt.subplots(1, n_plots, figsize=(6 * n_plots, 5), squeeze=False)\n",
        "\n",
        "for (name, params), ax in zip(configuracoes.items(), axes[0]):\n",
        "    umap = UMAP(\n",
        "        n_components=2,\n",
        "        random_state=42,\n",
        "        **params,\n",
        "    )\n",
        "\n",
        "    X_umap = umap.fit_transform(X)\n",
        "\n",
        "    ax.scatter(\n",
        "        X_umap[:, 0],\n",
        "        X_umap[:, 1],\n",
        "        s=40,\n",
        "        alpha=0.8,\n",
        "    )\n",
        "\n",
        "    ax.set_title(\n",
        "        f\"{name}\\nn_neighbors={params['n_neighbors']}, min_dist={params['min_dist']}\"\n",
        "    )\n",
        "    ax.set_xlabel(\"UMAP 1\")\n",
        "    ax.set_ylabel(\"UMAP 2\")\n",
        "    ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea498c4f",
      "metadata": {
        "id": "ea498c4f"
      },
      "source": [
        "## Classificação\n",
        "\n",
        "Com base nas categorias observadas nos gráficos anteriores, crie uma função simples que receba um texto e classifique-o na categoria mais provável.\n",
        "\n",
        "**Tarefas:**\n",
        "- Use os embeddings existentes e os clusters identificados para rotular automaticamente cada texto.  \n",
        "- Crie uma função `classificar_texto(texto: str)` que:\n",
        "  1. Gere o embedding do texto.\n",
        "  2. Calcule a distância para os clusters identificados.\n",
        "  3. Retorne o nome do cluster mais próximo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20d745d",
      "metadata": {
        "id": "a20d745d"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dbscan = DBSCAN(\n",
        "    eps=0.5,\n",
        "    min_samples=5\n",
        ")\n",
        "\n",
        "labels = dbscan.fit_predict(X_umap)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(\n",
        "    X_umap[:, 0],\n",
        "    X_umap[:, 1],\n",
        "    c=labels,\n",
        "    s=40,\n",
        "    alpha=0.8,\n",
        ")\n",
        "plt.title(\"DBSCAN em UMAP (2D)\")\n",
        "plt.xlabel(\"UMAP 1\")\n",
        "plt.ylabel(\"UMAP 2\")\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.3)\n",
        "plt.colorbar(scatter, label=\"Cluster (label)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "df_clusters = pd.DataFrame({\n",
        "    \"frase\": sentences,\n",
        "    \"cluster_id\": labels\n",
        "})\n",
        "\n",
        "cluster_ids_validos = np.unique(labels)\n",
        "cluster_ids_validos = cluster_ids_validos[cluster_ids_validos != -1]\n",
        "\n",
        "centroides = {}\n",
        "for c in cluster_ids_validos:\n",
        "    centroides[c] = X[labels == c].mean(axis=0)\n",
        "\n",
        "def classificar_texto(texto: str):\n",
        "    emb = modelo.encode([texto], convert_to_numpy=True)\n",
        "\n",
        "    ids = list(centroides.keys())\n",
        "    matriz_centroides = np.stack([centroides[c] for c in ids])\n",
        "\n",
        "    sims = cosine_similarity(emb, matriz_centroides)[0]\n",
        "\n",
        "    idx_melhor = sims.argmax()\n",
        "    cluster_escolhido = ids[idx_melhor]\n",
        "    similaridade = sims[idx_melhor]\n",
        "\n",
        "    nome_cluster = f\"Cluster {cluster_escolhido}\"\n",
        "\n",
        "    return {\n",
        "        \"cluster_id\": cluster_escolhido,\n",
        "        \"nome_cluster\": nome_cluster,\n",
        "        \"similaridade\": float(similaridade),\n",
        "        \"todas_similaridades\": dict(zip(ids, sims))\n",
        "    }\n",
        "\n",
        "testes = [\n",
        "    \"I like to cook Italian pasta with tomato sauce.\",\n",
        "    \"The Nile is one of the longest rivers in the world.\",\n",
        "    \"Stock market volatility affects portfolio risk.\",\n",
        "    \"Transformers are powerful models for NLP tasks.\",\n",
        "]\n",
        "\n",
        "for t in testes:\n",
        "    resultado = classificar_texto(t)\n",
        "    print(f\"\\nTexto: {t}\")\n",
        "    print(f\" → {resultado['nome_cluster']} (id = {resultado['cluster_id']}, sim = {resultado['similaridade']:.3f})\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}